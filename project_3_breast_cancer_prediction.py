# -*- coding: utf-8 -*-
"""Project_3_Breast_Cancer_Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eu6YMBBzEY00irmZ36u1Sdth6ao9BEn5

###Breast Cancer Prediction
Data Preprocessing

Exploratory Data Analysis

Feature Selection

Data Scaling

Various ML Algorithm

Hyperparameter Tuning

Model comparison

###Importing Libraries
"""

import pandas as pd
import numpy as np   #for multi matrix calculation
import seaborn as sns #for data visualization
import matplotlib.pyplot as plt #for data visualization
import missingno as msno #for missing values on dataset and categorical value
import warnings
warnings.filterwarnings('ignore')
sns.set()
plt.style.use('ggplot')

d=pd.read_csv('/content/data.csv')

d.head()

"""###Data preprocessing"""

d.diagnosis.unique()   #m-maligant,b-benin

"""If target column is given then it is supervised else it is unsupervised"""

d.describe()

d.info()

#check for missing values
d.isna().sum()

msno.bar(d,color='red') #there are no missing values in the dataset

d['diagnosis']=d['diagnosis'].apply(lambda val:1 if val=='M' else 0)

plt.hist(d['diagnosis'])
plt.title('Diagnosis(M=1,B=0)')
plt.show()

"""###EDA
Density Graph
"""

#each 5 rows it is having 6 column in 6th column it have density value
plt.figure(figsize=(20,15))
plotnumber=1
for column in d:
  if plotnumber<=30:
    ax=plt.subplot(5,6,plotnumber)
    sns.distplot(d[column])
    plt.xlabel(column)
  plotnumber+=1

plt.tight_layout()
plt.show()

d.corr()  #converting diagnosis value to numerical value

"""###heatmap"""

plt.figure(figsize=(20,12))
sns.heatmap(d.corr())
plt.show()

plt.figure(figsize=(20,12))
corr=d.corr()
mask=np.triu(np.ones_like(corr,dtype=bool))
sns.heatmap(corr,mask=mask,linewidths=1,annot=True,fmt=".2f")
plt.show()

"""###highly correlated feature - multicollinearity"""

d.drop('id',axis=1,inplace=True)

corr_matrix=d.corr().abs()
mask=np.triu(np.ones_like(corr_matrix,dtype=bool))
tri_df=corr_matrix.mask(mask)
to_drop=[x for x in tri_df.columns if any(tri_df[x]>0.92)] #values greater than 92% are dropped from actual dataset

d=d.drop(to_drop,axis=1)
print(d.shape[1])    #32 feature is reduce it 23 now

d.head()

to_drop

x=d.drop('diagnosis',axis=1)
y=d['diagnosis']

"""###Split train and test data"""

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=0)

#scaling data
from sklearn.preprocessing import StandardScaler
scaler=StandardScaler()
X_train=scaler.fit_transform(X_train)
X_test=scaler.transform(X_test)

X_train.shape

"""### Apply ML algo

###Logistic Regression
"""

from sklearn.linear_model import LogisticRegression
log_reg=LogisticRegression()
log_reg.fit(X_train,y_train)

y_pred=log_reg.predict(X_test)

y_pred

from sklearn.metrics import accuracy_score,confusion_matrix,classification_report
print(accuracy_score(y_train,log_reg.predict(X_train)))
log_reg_acc=accuracy_score(y_test,log_reg.predict(X_test))
print(log_reg_acc)

y_pred=log_reg.predict(X_test)
print(confusion_matrix(y_test,y_pred))

print(classification_report(y_test,y_pred))

"""###KNN"""

from sklearn.neighbors import KNeighborsClassifier
knn=KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train,y_train)

y_pred=knn.predict(X_test)

y_pred

from sklearn.metrics import accuracy_score,confusion_matrix,classification_report
print(accuracy_score(y_train,knn.predict(X_train)))
knn_acc=accuracy_score(y_test,knn.predict(X_test))
print(knn_acc)

y_pred=knn.predict(X_test)
print(confusion_matrix(y_test,y_pred))

print(classification_report(y_test,y_pred))

"""###SVM - Hyperparameter Tuning"""

from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV
svc=SVC(probability=True)
parameters={
    'gamma':[0.0001,0.001,0.01,0.1],
    'C':[0.01,0.05,0.5,0.1,1,10,15,20]
}
grid_search=GridSearchCV(svc,parameters)
grid_search.fit(X_train,y_train)

grid_search.best_params_

grid_search.best_score_

svc=SVC(C=15,gamma=0.01,probability=True)
svc.fit(X_train,y_train)

y_pred=svc.predict(X_test)

y_pred

from sklearn.metrics import accuracy_score,confusion_matrix,classification_report
print(accuracy_score(y_train,svc.predict(X_train)))
svc_acc=accuracy_score(y_test,svc.predict(X_test))
print(svc_acc)

y_pred=svc.predict(X_test)
print(confusion_matrix(y_test,y_pred))

print(classification_report(y_test,y_pred))

"""###Decision Tree"""

from sklearn.tree import DecisionTreeClassifier
dt=DecisionTreeClassifier()
parameters={
    'criterion':['gini','entropy'],
    'max_depth':range(2,32,1),
    'min_samples_leaf':range(1,10,1),
    'min_samples_split':range(2,10,1),
    'splitter':['best','random']
}

grid_search_dt=GridSearchCV(dt,parameters,cv=5,n_jobs=-1,verbose=1)
grid_search_dt.fit(X_train,y_train)

grid_search_dt.best_params_

grid_search_dt.best_score_

dtc=DecisionTreeClassifier(criterion='entropy',max_depth=23,min_samples_leaf=4,min_samples_split=4,splitter='random')

dtc.fit(X_train,y_train)

from sklearn.metrics import accuracy_score,confusion_matrix,classification_report
print(accuracy_score(y_train,dtc.predict(X_train)))
dtc_acc=accuracy_score(y_test,dtc.predict(X_test))
print(dtc_acc)

y_pred=dtc.predict(X_test)
print(confusion_matrix(y_test,y_pred))

print(classification_report(y_test,y_pred))

"""###Random forest classifier"""

from sklearn.ensemble import RandomForestClassifier
rand_clf=RandomForestClassifier(criterion='entropy',max_depth=10,max_features=0.5,min_samples_leaf=2,min_samples_split=3,n_estimators=130)
rand_clf.fit(X_train,y_train)

y_pred=rand_clf.predict(X_test)

from sklearn.metrics import accuracy_score,confusion_matrix,classification_report
print(accuracy_score(y_train,rand_clf.predict(X_train)))

rand_clf_acc=accuracy_score(y_test,rand_clf.predict(X_test))
print(rand_clf_acc)

y_pred=rand_clf.predict(X_test)
print(confusion_matrix(y_test,y_pred))

print(classification_report(y_test,y_pred))

"""###Gradient Boosting Classifier"""

from sklearn.ensemble import GradientBoostingClassifier
gb=GradientBoostingClassifier()
parameters={
    'loss':['deviance','exponential'],
    'learning_rate':[0.001,0.1],
    'n_estimators':[100,150,180]
}
grid_search_gb=GridSearchCV(gb,parameters,cv=2,n_jobs=-5,verbose=1)
grid_search_gb.fit(X_train,y_train)

grid_search_gb.best_params_

grid_search_gb.best_score_

gb=GradientBoostingClassifier(learning_rate=0.1,loss='exponential',n_estimators=180)
gb.fit(X_train,y_train)

from sklearn.metrics import accuracy_score,confusion_matrix,classification_report
print(accuracy_score(y_train,gb.predict(X_train)))

gb_acc=accuracy_score(y_test,gb.predict(X_test))
print(gb_acc)

y_pred=gb.predict(X_test)
print(confusion_matrix(y_test,y_pred))

print(classification_report(y_test,y_pred))

"""###XGB Classifier"""

from xgboost import XGBClassifier
xgb=XGBClassifier(objective='binary:logistic',learning_rate=0.01,max_depth=5,n_estimators=100)
xgb.fit(X_train,y_train)

from sklearn.metrics import accuracy_score,confusion_matrix,classification_report
print(accuracy_score(y_train,xgb.predict(X_train)))

xgb_acc=accuracy_score(y_test,xgb.predict(X_test))
print(xgb_acc)

y_pred=xgb.predict(X_test)
print(confusion_matrix(y_test,y_pred))

print(classification_report(y_test,y_pred))

"""###Model comparison"""

models=pd.DataFrame({
    'Model':['Logistic Regression','KNN','SVM','Decision Tree Classifier','Random Forest Classifier','Gradient Boosting','XGBoost'],
    'Score':[100*round(log_reg_acc,4),100*round(knn_acc,4),100*round(svc_acc,4),100*round(dtc_acc,4),
             100*round(rand_clf_acc,4),100*round(gb_acc,4),100*round(xgb_acc,4)]
})
models.sort_values(by='Score',ascending=False)

import pickle
model=svc
pickle.dump(model,open("data.pkl","wb"))

from sklearn import metrics
plt.figure(figsize=(8,5))
models=[
    {
        'label':'LR',
        'model':log_reg,
    },
    {
        'label':'DT',
        'model':dtc,
    },
    {
        'label':'SVM',
        'model':svc,
    },
    {
        'label':'KNN',
        'model':knn,
    },
    {
        'label':'XGBoost',
        'model':xgb,
    },
    {
        'label':'RF',
        'model':rand_clf,
    },
    {
        'label':'GBDT',
        'model':gb,
    }
]
for m in models:
  model=m['model']
  model.fit(X_train,y_train)
  y_pred=model.predict(X_test)
  fpr1, tpr1, thresholds=metrics.roc_curve(y_test,model.predict_proba(X_test)[:,1])
  auc=metrics.roc_auc_score(y_test,model.predict(X_test))
  plt.plot(fpr1,tpr1,label='%s - ROC (area = %0.2f)' % (m['label'], auc))

plt.plot([0,1],[0,1],'r--')
plt.xlim([-0.01,1.0])
plt.ylim([0.0,1.05])
plt.xlabel('1 - Specificity (False Positive Rate)',fontsize=12)
plt.ylabel('Sensitivity (True Positive Rate)',fontsize=12)
plt.title('Receiver Operating Characteristic ROC - Breast Cancer Prediction',fontsize=12)
plt.legend(loc='lower right',fontsize=12)
plt.savefig("roc_breast_cancer.jpeg",format='jpeg',dpi=400,bbox_inches='tight')
plt.show()

from sklearn import metrics
plt.figure(figsize=(8,5))
models=[
    {
        'label':'LR',
        'model':log_reg,
    },
    {
        'label':'DT',
        'model':dtc,
    },
    {
        'label':'SVM',
        'model':svc,
    },
    {
        'label':'KNN',
        'model':knn,
    },
    {
        'label':'XGBoost',
        'model':xgb,
    },
    {
        'label':'RF',
        'model':rand_clf,
    },
    {
        'label':'GBDT',
        'model':gb,
    }
]

means_roc=[]
means_accuracy=[100*round(log_reg_acc,4),100*round(knn_acc,4),100*round(svc_acc,4),100*round(dtc_acc,4),
                100*round(rand_clf_acc,4),100*round(gb_acc,4),100*round(xgb_acc,4)]
for m in models:
  model=m['model']
  model.fit(X_train,y_train)
  y_pred=model.predict(X_test)
  fpr1, tpr1, thresholds=metrics.roc_curve(y_test,model.predict_proba(X_test)[:,1])
  auc=metrics.roc_auc_score(y_test,model.predict(X_test))
  auc=100*round(auc,4)
  means_roc.append(auc)

print(means_accuracy)
print(means_roc)

#data to plot
n_groups=7
means_accuracy=tuple(means_accuracy)
means_roc=tuple(means_roc)

#create plot
fig,ax=plt.subplots(figsize=(8,5))
index=np.arange(n_groups)
bar_width=0.35
opacity=0.8

rects1=plt.bar(index,means_accuracy,bar_width,alpha=opacity,color='lightgreen',label='Accuracy (%)')
rects2=plt.bar(index + bar_width,means_roc,bar_width,alpha=opacity,color='green',label='ROC (%)')

plt.xlim([-1,8])
plt.ylim([70,104])

plt.title('Performance Evaluation',fontsize=12)
plt.xticks(index,('  LR','  DT','  SVM','  KNN','  XGBoost','  RF','  GBDT'),rotation=40,fontsize=12)
plt.legend(loc="upper right",fontsize=10)
plt.savefig("performance_evaluation.jpeg",format='jpeg',dpi=400,bbox_inches='tight')
plt.show()